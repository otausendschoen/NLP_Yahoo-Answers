# Few-Shot Text Classification with BERT and LLaMA-Augmented Data

This repository contains the full pipeline for an NLP classification project aimed at robust text categorization with minimal labeled data. The project evaluates several strategies for improving model performance in low-resource settings by leveraging both rule-based and large language model (LLM) techniques.


- Benchmark performance of **BERT-based** models under few-shot constraints
- Evaluate effectiveness of **synthetic data augmentation** using **LLaMA**
- Implement **zero-shot** and **semi-supervised learning** techniques
- Compare all approaches to state-of-the-art results using full datasets


### 1. Dataset & Baseline
- Chosen from the HuggingFace hub: (yahoo_answers_topics)
- Dataset statistics, random classifier expectations, and rule-based baseline implemented.

### 2. Data Scientist Challenge
- **32-shot BERT** classifier
- **Geometric and LLaMA-based data augmentation**
- **Zero-shot inference** using ChatGPT/LLaMA
- **Label generation** with LLaMA to enhance training set
- Comparison of each approach via metrics like F1-score, Accuracy

### 3. Full Dataset Training
- Learning curve across [1%, 10%, 25%, 50%, 75%, 100%] dataset splits
- Evaluation of hybrid training strategies combining synthetic + real data
- Part A and B are in notebook "Part3_AB.ipynb" which was run on Kaggle in Kaggle for computational requirements.
- Part C and D are in notebook "Part3_CD.ipynb" which was executed on Google Colab using T4 GPUs.

### 4. Model Optimization
- Distillation and quantization of the best BERT model
- Inference speed and performance trade-off analysis




---

## Repository Structure

| File | Description |
|------|-------------|
| `Part_1-2.ipynb` | Part 1 and 2
| `Part3_AB.ipynb` | First Half of Part 3, includes Task A & B
| `Part3_CD.ipynb` | Second Half of Part 3, includes Task C & D
| `Part_4.ipynb` | Part 4
| `questions_dataset.csv`  | The syhnthetic dataset with the questions generated with llama3.2
| `ZeroShotClassification.csv`  |The dataset with the questions classified with the llama3.2 (ZeroShot

---

# Exectutive Summary:

**Purpose and Scope**

This project explores strategies to enhance text classification performance in low-resource scenarios, where labeled data is limited. The goal is to develop a robust NLP pipeline by combining rule-based methods with large language model (LLM) techniques. The scope includes benchmarking BERT-based models under few-shot settings, testing synthetic data augmentation using LLaMA, and implementing zero-shot and semi-supervised learning approaches.
We used the Random Classifier as a bottom benchmark, where we obtained 0.1002 in the accuracy metrics.

**Findings**
- When trying to enhance the model with limited data, we observe that there is no significant difference between using a stratified sample of the 32 labeled examples and selecting them randomly. The results are very similar, likely because with such a small amount of data, the model cannot learn effectively. This may be due to the diversity in question types in terms of both structure and content, as the dataset is human-generated. With so few examples, the model struggles to generalize. In both the stratified and unstratified cases, performance is only slightly better than random classification—by approximately 0.14.
  
- When applying data augmentation techniques, there is a clear difference in performance between using LLM-generated data and automated techniques that do not rely on LLMs. Specifically, when we used masked token replacement to generate new data, the results were quite poor. With only one masked token per sentence, the model's performance was close to random classification. However, increasing to three masked tokens per sentence improved accuracy to 0.15. This improvement may be due to the larger amount of training data, allowing the model to learn more effectively.
Still, these results remain relatively low compared to those obtained using data generated by a large language model. Using LLaMA 3.2, we synthetically generated 10,000 new questions. Interestingly, the model did not require the full dataset to reach optimal performance—training with just 6,000 generated samples led to the best results while also reducing computational cost.
Overall, the LLM-based augmentation clearly outperformed the non-LLM approach, achieving 0.50 accuracy, 0.54 precision, and 0.49 recall, which were the best results obtained in Part 2 of the project.
<img src="https://github.com/user-attachments/assets/9c4b2b35-3cb0-459a-b9dc-544de37537a0" width="400"/>
<img src="https://github.com/user-attachments/assets/62a6f0cc-526f-411d-9d84-318c01492e80" width="400"/>




- Using an LLM for zero-shot learning, we observed that the model's performance varied significantly across different classification topics. In some cases, it performed very well, accurately identifying topics and achieving high recall. However, in other cases, the performance was poor. This inconsistency suggests that the model struggles to generalize uniformly across all topics. As a result, the overall metrics reflect this variability: 0.257 accuracy, 0.44 precision, and 0.256 recall. The model struggled to consistently identify the correct topic.
 




- Learning curves were analyzed using dataset splits of [1%, 5%, 10%, 25%]. While technical issues prevented retrieval of some training logs (especially for 10% and 25%), we preserved performance metrics (accuracy, precision, recall) and plotted the available validation losses over epochs.
The learning curves clearly show that models trained on larger data subsets reach lower and more stable validation loss. The 25% model, in particular, converged rapidly and had the flattest curve, suggesting efficient learning. 
However, visual extrapolation of the loss curves also suggests that the model gains diminish gradually after 10%, supporting the assumption of diminishing returns on additional data past a certain point.
<img src="https://github.com/user-attachments/assets/598acdde-ae50-4603-88f4-b2d975009687" width="600"/>
<img src="https://github.com/user-attachments/assets/c1256529-47e1-463b-a521-f4fdd07d53de" width="600"/>


- When doing distillation and quantization to compress the fine-tuned BERT model trained on 5% of the data, we observed substantial inference speedups while retaining much of the classification performance. For example, our quantized student model achieved 65.27% accuracy, while cutting inference time in half (from ~0.70s to ~0.3s per 100 samples). This demonstrates that model compression via quantization can maintain effectiveness while improving deployability in low-resource environments.

- Other distilled student variants showed competitive and similiar performance. The pretrained student model (standard knowledge distillation using soft labels) reached 66.5% accuracy, while the intermediate-distilled student (with TinyBERT-style hidden layer alignment) achieved 66.08%. However, the original teacher model still outperformed all distilled versions, with an accuracy of 66.28% on the same test split.

- A per-class accuracy analysis revealed that the student retained strong performance across nearly all topics, closely mirroring the teacher model.

- In summary, distillation is a highly effective compression method in our pipeline, achieving fast, accurate, and lightweight models. However, careful attention must be given to the training strategy (e.g., use of soft labels), and further improvements could involve layer-sharing, intermediate feature alignment, or selective knowledge transfer to better match teacher predictions on complex samples.
  
**General Summary of Models**
| Part      | Model / Method                                      | Accuracy (%) |
|-----------|-----------------------------------------------------|--------------|
| **Part 1** | Random Classifier                                   | 9.99         |
|           | Baseline Implementation                              | 21.25        |
| **Part 2** | 32-shot BERT 1.0 (no class balancing)               | 13.1         |
|           | 32-shot BERT 2.0 (class balancing)                   | 13.6         |
|           | + 1-Masked Token Augmentation                        | 11.1         |
|           | + 3-Masked Token Augmentation                        | 14.4         |
|           | Zero-shot LLM Classification                         | 25.7         |
|           | LLM-Generated Data (5,000 examples)                  | 50.0         |
|           | LLM-Generated Data (6,000 examples)                  | 50.7         |
|           | LLM-Generated Data (10,000 examples)                 | 51.4         |
| **Part 3** | BERT on 1% of data                                  | 63.6         |
|           | BERT on 5% of data                                   | 67.5         |
|           | BERT on 10% of data                                  | 67.2         |
|           | BERT on 25% of data                                  | 68.3         |
|           | + 1-Mask Aug. (5% data)                              | 68.0         |
|           | + 3-Mask Aug. (5% data)                              | 67.0         |
| **Part 4** | Teacher Model (Fine-tuned BERT on 5%)               | 66.28         |
|           | Student (Pre-trained only)                           | 66.5         |
|           | Student (Intermediate Distilled)                     | 66.08         |
|           | Student v1 (Distilled from Teacher)                  | 65.15         |
|           | Quantized Classifier                                 | 65.27         |
  
