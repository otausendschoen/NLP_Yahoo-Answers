# Few-Shot Text Classification with BERT and LLaMA-Augmented Data

This repository contains the full pipeline for an NLP classification project aimed at robust text categorization with minimal labeled data. The project evaluates several strategies for improving model performance in low-resource settings by leveraging both rule-based and large language model (LLM) techniques.


- Benchmark performance of **BERT-based** models under few-shot constraints
- Evaluate effectiveness of **synthetic data augmentation** using **LLaMA**
- Implement **zero-shot** and **semi-supervised learning** techniques
- Compare all approaches to state-of-the-art results using full datasets


### 1. Dataset & Baseline
- Chosen from the HuggingFace hub: (yahoo_answers_topics)
- Dataset statistics, random classifier expectations, and rule-based baseline implemented.

### 2. Data Scientist Challenge
- **32-shot BERT** classifier
- **Geometric and LLaMA-based data augmentation**
- **Zero-shot inference** using ChatGPT/LLaMA
- **Label generation** with LLaMA to enhance training set
- Comparison of each approach via metrics like F1-score, Accuracy

### 3. Full Dataset Training
- Learning curve across [1%, 10%, 25%, 50%, 75%, 100%] dataset splits
- Evaluation of hybrid training strategies combining synthetic + real data
- Part A and B are in notebook "Part3_AB.ipynb" which was run on Kaggle in Kaggle for computational requirements.
- Part C and D are in notebook "Part3_CD.ipynb" which was executed on Google Colab using T4 GPUs.

### 4. Model Optimization
- Distillation and quantization of the best BERT model
- Inference speed and performance trade-off analysis




---

## Repository Structure

| File | Description |
|------|-------------|
| `Part_1-2.ipynb` | Part 1 and 2
| `Part_3_A-B.ipynb` | First Half of Part 3, includes Task A & B
| `Part_4.ipynb` | Part 4
| `.csv` files |?

---

## Exectutive Summary:
**Purpose and Scope **
This project explores strategies to enhance text classification performance in low-resource scenarios, where labeled data is limited. The goal is to develop a robust NLP pipeline by combining rule-based methods with large language model (LLM) techniques. The scope includes benchmarking BERT-based models under few-shot settings, testing synthetic data augmentation using LLaMA, and implementing zero-shot and semi-supervised learning approaches.
We used the Random Classifier as a bottom benchmark, where we obtained 0.1002 in the accuracy metrics.

**Findings**
- When trying to enhance the model with limited data, we observe that there is no significant difference between using a stratified sample of the 32 labeled examples and selecting them randomly. The results are very similar, likely because with such a small amount of data, the model cannot learn effectively. This may be due to the diversity in question types in terms of both structure and content, as the dataset is human-generated. With so few examples, the model struggles to generalize. In both the stratified and unstratified cases, performance is only slightly better than random classification—by approximately 0.14.
  
- When applying data augmentation techniques, there is a clear difference in performance between using LLM-generated data and automated techniques that do not rely on LLMs. Specifically, when we used masked token replacement to generate new data, the results were quite poor. With only one masked token per sentence, the model's performance was close to random classification. However, increasing to three masked tokens per sentence improved accuracy to 0.15. This improvement may be due to the larger amount of training data, allowing the model to learn more effectively.
Still, these results remain relatively low compared to those obtained using data generated by a large language model. Using LLaMA 3.2, we synthetically generated 10,000 new questions. Interestingly, the model did not require the full dataset to reach optimal performance—training with just 6,000 generated samples led to the best results while also reducing computational cost.
Overall, the LLM-based augmentation clearly outperformed the non-LLM approach, achieving 0.50 accuracy, 0.54 precision, and 0.49 recall, which were the best results obtained in Part 2 of the project.
<img src="https://github.com/user-attachments/assets/9c4b2b35-3cb0-459a-b9dc-544de37537a0" width="400"/>
<img src="https://github.com/user-attachments/assets/5173e50b-f3d7-421c-8b86-a68f7e1e64f9" width="400"/>




- Using an LLM for zero-shot learning, we observed that the model's performance varied significantly across different classification topics. In some cases, it performed very well, accurately identifying topics and achieving high recall. However, in other cases, the performance was poor. This inconsistency suggests that the model struggles to generalize uniformly across all topics. As a result, the overall metrics reflect this variability: 0.37 accuracy, 0.55 precision, and 0.37 recall. The model struggled to consistently identify the correct topic.
 
<img src="https://github.com/user-attachments/assets/092a0126-d232-40e3-adf5-f3e655207781" width="400"/>



Learning curves were analyzed using dataset splits of [1%, 5%, 10%, 25%]. While technical issues prevented retrieval of some training logs (especially for 10% and 25%), we preserved performance metrics (accuracy, precision, recall) and plotted the available validation losses over epochs.
The learning curves clearly show that models trained on larger data subsets reach lower and more stable validation loss. The 25% model, in particular, converged rapidly and had the flattest curve, suggesting efficient learning. 
However, visual extrapolation of the loss curves also suggests that the model gains diminish gradually after 10%, supporting the assumption of diminishing returns on additional data past a certain point.
![image](https://github.com/user-attachments/assets/598acdde-ae50-4603-88f4-b2d975009687)

