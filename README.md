# Few-Shot Text Classification with BERT and LLaMA-Augmented Data

This repository contains the full pipeline for an NLP classification project aimed at robust text categorization with minimal labeled data. The project evaluates several strategies for improving model performance in low-resource settings by leveraging both rule-based and large language model (LLM) techniques.


- Benchmark performance of **BERT-based** models under few-shot constraints
- Evaluate effectiveness of **synthetic data augmentation** using **LLaMA**
- Implement **zero-shot** and **semi-supervised learning** techniques
- Compare all approaches to state-of-the-art results using full datasets


### 1. Dataset & Baseline
- Chosen from the HuggingFace hub: (yahoo_answers_topics)
- Dataset statistics, random classifier expectations, and rule-based baseline implemented.

### 2. Data Scientist Challenge
- **32-shot BERT** classifier
- **Geometric and LLaMA-based data augmentation**
- **Zero-shot inference** using ChatGPT/LLaMA
- **Label generation** with LLaMA to enhance training set
- Comparison of each approach via metrics like F1-score, Accuracy

### 3. Full Dataset Training
- Learning curve across [1%, 10%, 25%, 50%, 75%, 100%] dataset splits
- Evaluation of hybrid training strategies combining synthetic + real data
- Part A and B are in notebook "Part3_AB.ipynb" which was run on Kaggle in Kaggle for computational requirements.
- Part C and D are in notebook "Part3_CD.ipynb" which was executed on Google Colab using T4 GPUs.

### 4. Model Optimization
- Distillation and quantization of the best BERT model
- Inference speed and performance trade-off analysis




---

## Repository Structure

| File | Description |
|------|-------------|
| `Part_1-2.ipynb` | Part 1 and 2
| `Part3_AB.ipynb` | First Half of Part 3, includes Task A & B
| `Part3_CD.ipynb` | Second Half of Part 3, includes Task C & D
| `Part_4.ipynb` | Part 4
| `questions_dataset.csv`  | The syhnthetic dataset with the questions generated with llama3.2
| `ZeroShotClassification.csv`  |The dataset with the questions classified with the llama3.2 (ZeroShot)
| `training_content_title.ipynb`  |This includes complementary training on 5% and 20% dataset using both content and title as features

---

# Exectutive Summary:

**Purpose and Scope**

This project explores strategies to enhance text classification performance in low-resource scenarios, where labeled data is limited. The goal is to develop a robust NLP pipeline by combining rule-based methods with large language model (LLM) techniques. The scope includes benchmarking BERT-based models under few-shot settings, testing synthetic data augmentation using LLaMA, and implementing zero-shot and semi-supervised learning approaches.
We used the Random Classifier as a bottom benchmark, where we obtained 0.1002 in the accuracy metrics.

**Findings**
- When trying to enhance the model with limited data, we observe that there is no significant difference between using a stratified sample of the 32 labeled examples and selecting them randomly. The results are very similar, likely because with such a small amount of data, the model cannot learn effectively. This may be due to the diversity in question types in terms of both structure and content, as the dataset is human-generated. With so few examples, the model struggles to generalize. In both the stratified and unstratified cases, performance is only slightly better than random classification—by approximately 0.14.
  
- When applying data augmentation techniques, there is a clear difference in performance between using LLM-generated data and automated techniques that do not rely on LLMs. Specifically, when we used masked token replacement to generate new data, the results were quite poor. With only one masked token per sentence, the model's performance was close to random classification. However, increasing to three masked tokens per sentence improved accuracy to 0.15. This improvement may be due to the larger amount of training data, allowing the model to learn more effectively.
Still, these results remain relatively low compared to those obtained using data generated by a large language model. Using LLaMA 3.2, we synthetically generated 10,000 new questions. Interestingly, the model did not require the full dataset to reach optimal performance—training with just 6,000 generated samples led to the best results while also reducing computational cost.
Overall, the LLM-based augmentation clearly outperformed the non-LLM approach, achieving 0.50 accuracy, 0.54 precision, and 0.49 recall, which were the best results obtained in Part 2 of the project.
<img src="https://github.com/user-attachments/assets/9c4b2b35-3cb0-459a-b9dc-544de37537a0" width="400"/>
<img src="https://github.com/user-attachments/assets/62a6f0cc-526f-411d-9d84-318c01492e80" width="400"/>




- Using an LLM for zero-shot learning, we observed that the model's performance varied significantly across different classification topics. In some cases, it performed very well, accurately identifying topics and achieving high recall. However, in other cases, the performance was poor. This inconsistency suggests that the model struggles to generalize uniformly across all topics. As a result, the overall metrics reflect this variability: 0.257 accuracy, 0.44 precision, and 0.256 recall. The model struggled to consistently identify the correct topic.
 




- Learning curves were analyzed using dataset splits of [1%, 5%, 10%, 25%]. While technical issues prevented retrieval of some training logs (especially for 10% and 25%), we preserved performance metrics (accuracy, precision, recall) and plotted the available validation losses over epochs.
The learning curves clearly show that models trained on larger data subsets reach lower and more stable validation loss. The 25% model, in particular, converged rapidly and had the flattest curve, suggesting efficient learning. 
However, visual extrapolation of the loss curves also suggests that the model gains diminish gradually after 10%, supporting the assumption of diminishing returns on additional data past a certain point.
<img src="https://github.com/user-attachments/assets/598acdde-ae50-4603-88f4-b2d975009687" width="600"/>
<img src="https://github.com/user-attachments/assets/c1256529-47e1-463b-a521-f4fdd07d53de" width="600"/>


- When applying distillation and quantization to compress the fine-tuned BERT model trained on a reduced 5% subset of the dataset, we were able to retain most of the classification performance while significantly reducing model size and improving inference efficiency, particularly in CPU-constrained environments.

- The best student model, which uses intermediate layer distillation (with TinyBERT-style hidden state alignment), slightly outperformed the teacher, achieving an accuracy of **66.54%** compared to the teacher’s **66.28%**. This result demonstrates that carefully guided student training can even surpass the teacher in terms of generalization, despite the student’s smaller capacity. The standard distilled student, using only soft targets, reached **65.27%**, showing competitive performance with a simpler objective.

- Quantization was applied post-training using ONNX Runtime, reducing the weight precision from 32-bit floats to 8-bit integers. While this did not improve inference speed on GPU, due to overheads and partial operator support, it did significantly reduce latency on CPU. Inference time on 100 samples dropped from around **10.8 seconds for the teacher** to just **2.5 seconds for the quantized student ONNX model**, demonstrating its practicality for edge deployment.

- A per-class accuracy comparison confirmed that the intermediate student preserved performance across nearly all categories, showing particular robustness in classes such as *Society & Culture* and *Family & Relationships*. The confusion matrix revealed a strong alignment with the teacher’s predictions, though minor deviations still existed in more ambiguous classes.

- In summary, model distillation proved highly effective for compressing and accelerating the BERT-based classifier without sacrificing accuracy. The inclusion of intermediate feature alignment clearly improved performance beyond standard soft-label distillation. Further improvements could include combining this strategy with quantization-aware training or mixed-precision optimization to enable faster inference on both GPU and CPU, as well as investigating hybrid architectures with adaptive depth or selective layer reuse from the teacher.

**General Summary of Models**
| Part      | Model / Method                                      | Accuracy (%) |
|-----------|-----------------------------------------------------|--------------|
| **Part 1** | Random Classifier                                   | 9.99         |
|           | Baseline Implementation                              | 21.25        |
| **Part 2** | 32-shot BERT 1.0 (no class balancing)               | 13.1         |
|           | 32-shot BERT 2.0 (class balancing)                   | 13.6         |
|           | + 1-Masked Token Augmentation                        | 11.1         |
|           | + 3-Masked Token Augmentation                        | 14.4         |
|           | Zero-shot LLM Classification                         | 25.7         |
|           | LLM-Generated Data (5,000 examples)                  | 50.0         |
|           | LLM-Generated Data (6,000 examples)                  | 50.7         |
|           | LLM-Generated Data (10,000 examples)                 | 51.1         |
| **Part 3** | BERT on 1% of data                                  | 63.6         |
|           | BERT on 5% of data                                   | 67.5         |
|           | BERT on 10% of data                                  | 67.2         |
|           | BERT on 25% of data                                  | 68.3         |
|           | + 1-Mask Aug. (5% data)                              | 67.2         |
|           | + 2-Mask Aug. (5% data)                              | 66.9         |
| **Part 4** | Teacher Model (Fine-tuned BERT on 5%)               | 66.28         |
|           | Student (Intermediate Distilled)                     | 66.54         |
|           | Student (Pre-trained only)                           | 65.27         |
|           | Quantized Student (ONNX)                             | 65.19         |

  
