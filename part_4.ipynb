{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183990f3",
   "metadata": {},
   "source": [
    "![bse_logo_textminingcourse](https://bse.eu/sites/default/files/bse_logo_small.png)\n",
    "\n",
    "# *Yahoo Answers Topic Classification*\n",
    "\n",
    "### Authors: **Tirdod, Behbehani, Marvin Ernst, Pol Garcia, Oliver TausendschÃ¶n** \n",
    "\n",
    "#### Class: **22DM015 Advanced Methods in Natural Language Processing**\n",
    "\n",
    "##### *Final Assigment*\n",
    "\n",
    "##### Supervisor: **Arnault Gombert**\n",
    "\n",
    "**Date: June 15, 2025**\n",
    "\n",
    "\n",
    "# **Part 4: Model Distillation and Quantization**\n",
    "\n",
    "In this section, we aim to reduce the computational load of our best-performing model from Part 3. This is essential for efficient deployment in resource-constrained environments.\n",
    "\n",
    "We want to reduce computational cost, while retaining as much perfromance as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea981c",
   "metadata": {},
   "source": [
    "**Importing the relevant libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b11852",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AdamW' from 'transformers' (/opt/anaconda3/envs/NLPenv/lib/python3.11/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdamW\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'AdamW' from 'transformers' (/opt/anaconda3/envs/NLPenv/lib/python3.11/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826abaad",
   "metadata": {},
   "source": [
    "Set a  seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b1a3b0",
   "metadata": {},
   "source": [
    "## a. Model Distillation/Quantization \n",
    "\n",
    "**Goal**: Convert our large BERT-based model into a smaller and faster model using knowledge distillation and/or quantization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5315e026",
   "metadata": {},
   "source": [
    "### 1. Distillation Setup\n",
    "\n",
    "We will use the `distilbert-base-uncased` architecture as our student model. The teacher is our fine-tuned BERT model from Part 2, which was our best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435bbc99",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HF_TOKEN'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m login(token=\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHF_TOKEN\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m      6\u001b[39m teacher_ckpt = \u001b[33m\"\u001b[39m\u001b[33mtirdodbehbehani/yahoo-bert-32shot_stratified_augm_2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_ckpt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:679\u001b[39m, in \u001b[36m__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'HF_TOKEN'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "# Login to Hugging Face Hub\n",
    "\n",
    "teacher_ckpt = \"tirdodbehbehani/yahoo-bert-32shot_stratified_augm_2\"\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_ckpt)\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_ckpt)\n",
    "\n",
    "student_ckpt = \"distilbert-base-uncased\"\n",
    "student_model = DistilBertForSequenceClassification.from_pretrained(student_ckpt, num_labels=10)\n",
    "student_tokenizer = DistilBertTokenizerFast.from_pretrained(student_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e6886",
   "metadata": {},
   "source": [
    "### 2. Dataset Preparation\n",
    "\n",
    "We will use 10% of the dataset for distillation due to computational constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313776f8",
   "metadata": {},
   "source": [
    "First, we load the Yahoo dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "865f0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    load_dataset(\"community-datasets/yahoo_answers_topics\", split=\"train\").to_pandas(),\n",
    "    load_dataset(\"community-datasets/yahoo_answers_topics\", split=\"test\").to_pandas()\n",
    "])\n",
    "\n",
    "df = df[df[\"question_content\"].str.strip() != \"\"]\n",
    "df = df[df[\"best_answer\"].str.strip() != \"\"]\n",
    "df = df[[\"question_content\", \"topic\"]].rename(columns={\"topic\": \"label\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ed7e6",
   "metadata": {},
   "source": [
    "Sample 10%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40808d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample, _ = train_test_split(df, train_size=0.1, stratify=df[\"label\"], random_state=42)\n",
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbef9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {label: idx for idx, label in enumerate(sorted(df_sample[\"label\"].unique()))}\n",
    "df_sample[\"label\"] = df_sample[\"label\"].map(label_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4242c2e",
   "metadata": {},
   "source": [
    "Convert to Hugging Face Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "519d8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "502a091b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18d2d45ac004b79a9f3bc60de99f340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/78594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_student(example):\n",
    "    return student_tokenizer(example[\"question_content\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "dataset = dataset.map(tokenize_student, batched=True)\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25bb65",
   "metadata": {},
   "source": [
    "### 3. Distillation Training\n",
    "\n",
    "We use a simple loss function that encourages the student to match the teacher's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa854e12",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m device = \u001b[43mtorch\u001b[49m.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m teacher_model.to(device).eval()\n\u001b[32m      3\u001b[39m student_model.to(device).train()\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher_model.to(device).eval()\n",
    "student_model.to(device).train()\n",
    "\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=32, shuffle=True)\n",
    "optimizer = AdamW(student_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc25e02",
   "metadata": {},
   "source": [
    "We define a distillation loss that combines cross-entropy loss and Kullback-Leibler divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceac390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student_logits, teacher_logits, true_labels, alpha=0.5, temperature=2.0):\n",
    "    ce_loss = F.cross_entropy(student_logits, true_labels)\n",
    "    kl_loss = F.kl_div(\n",
    "        F.log_softmax(student_logits / temperature, dim=1),\n",
    "        F.softmax(teacher_logits / temperature, dim=1),\n",
    "        reduction=\"batchmean\"\n",
    "    ) * (temperature ** 2)\n",
    "    return alpha * ce_loss + (1 - alpha) * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        student_logits = student_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        loss = distillation_loss(student_logits, teacher_logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93f9e0",
   "metadata": {},
   "source": [
    "### 4. Quantization with ONNX or PyTorch Static Quantization\n",
    "# \n",
    "# - Quantization can further reduce model size and speed up inference.\n",
    "# - To implement: Use Hugging Face Optimum + ONNX export, or PyTorch FX graph for static quantization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6734e",
   "metadata": {},
   "source": [
    "## b. Performance and Speed Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104c6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.eval()\n",
    "test_loader = DataLoader(dataset[\"test\"], batch_size=64)\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(\"cpu\").numpy()\n",
    "\n",
    "        outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, axis=1).cpu().numpy()\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "prec = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "rec = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "print(f\"Student Model Accuracy: {acc:.4f}\")\n",
    "print(f\"Student Model Precision: {prec:.4f}\")\n",
    "print(f\"Student Model Recall: {rec:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42278e19",
   "metadata": {},
   "source": [
    "### Inference Speed Comparison\n",
    "\n",
    "Sample 100 texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d79b658",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_sample[\"question_content\"].tolist()[:100]\n",
    "inputs = student_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    _ = student_model(**inputs)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Inference time on 100 samples (student): {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfa75b",
   "metadata": {},
   "source": [
    "## c. Analysis and Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f975306",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
