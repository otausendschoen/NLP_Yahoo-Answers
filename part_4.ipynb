{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183990f3",
   "metadata": {},
   "source": [
    "![bse_logo_textminingcourse](https://bse.eu/sites/default/files/bse_logo_small.png)\n",
    "\n",
    "# *Yahoo Answers Topic Classification*\n",
    "\n",
    "### Authors: **Tirdod Behbehani, Marvin Ernst, Pol Garcia, Oliver Tausendschön** \n",
    "\n",
    "#### Class: **22DM015 Advanced Methods in Natural Language Processing**\n",
    "\n",
    "##### *Final Assigment*\n",
    "\n",
    "##### Supervisor: **Arnault Gombert**\n",
    "\n",
    "**Date: June 15, 2025**\n",
    "\n",
    "\n",
    "# **Part 4: Model Distillation and Quantization**\n",
    "\n",
    "In this section, we aim to reduce the computational load of our best-performing model from Part 3. This is essential for efficient deployment in resource-constrained environments.\n",
    "\n",
    "We want to reduce computational cost, while retaining as much perfromance as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea981c",
   "metadata": {},
   "source": [
    "**Importing the relevant libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b11852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DistilBertTokenizerFast, DistilBertConfig, DistilBertForSequenceClassification\n",
    "from transformers import BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "import copy\n",
    "\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826abaad",
   "metadata": {},
   "source": [
    "Set a  seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c9d3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b1a3b0",
   "metadata": {},
   "source": [
    "## a. Model Distillation/Quantization \n",
    "\n",
    "**Goal**: Convert our large BERT-based model into a smaller and faster model using knowledge distillation and/or quantization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5315e026",
   "metadata": {},
   "source": [
    "### **1. Distillation Setup**\n",
    "\n",
    "We will use the `distilbert-base-uncased` architecture as our student model. The teacher is our fine-tuned BERT model from Part 2, which was our best model.\n",
    "\n",
    "(For the teacher model we don't explicitly have to align label mapping to integer-based labels since they are already 0–9).\n",
    "\n",
    "Before we have been using hugging face, however, we encountered issues with it, thus we store the model locally, we will send you the zip file to the pretrained BERT model.\n",
    "\n",
    "This would be the hugging face option, which we still use for the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# Login to Hugging Face Hub\n",
    "#login(token=\".....\")\n",
    "\n",
    "teacher_ckpt_tokenizer = \"tirdodbehbehani/yahoo-bert-32shot_stratified_augm_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba425b29",
   "metadata": {},
   "source": [
    "This is when model is stored locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "435bbc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# the path to the pretrained model:\n",
    "teacher_ckpt = \"yahoo_models_best_checkpoints/results_5/checkpoint-2460\"\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_ckpt)\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_ckpt_tokenizer)\n",
    "\n",
    "student_ckpt = \"distilbert-base-uncased\"\n",
    "student_model = DistilBertForSequenceClassification.from_pretrained(student_ckpt, num_labels=10)\n",
    "student_tokenizer = DistilBertTokenizerFast.from_pretrained(student_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e6886",
   "metadata": {},
   "source": [
    "### **2. Dataset Preparation**\n",
    "\n",
    "We will use 5% of the dataset for distillation due to computational constraints, since the goal is here to investigate how we can do a best possible distillation, that still gets close in perfromance to the teacher's model (so we are also using the teacher's model that has been trained on 5% of the data). Of course one could also go for 100%, however the learnings would be similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313776f8",
   "metadata": {},
   "source": [
    "First, we load the Yahoo dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4cdae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    load_dataset(\"community-datasets/yahoo_answers_topics\", split=\"train\").to_pandas(),\n",
    "    load_dataset(\"community-datasets/yahoo_answers_topics\", split=\"test\").to_pandas()\n",
    "])\n",
    "\n",
    "df = df[df[\"question_content\"].str.strip() != \"\"]\n",
    "df = df[df[\"best_answer\"].str.strip() != \"\"]\n",
    "df = df[[\"question_content\", \"topic\"]].rename(columns={\"topic\": \"label\"})\n",
    "df[\"label\"] = df[\"label\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ed7e6",
   "metadata": {},
   "source": [
    "Sample 5%:\n",
    "\n",
    "(We had tried using a fixed label map consistent with teacher training, but this is redundant, since labels are already 0-9.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40808d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample, _ = train_test_split(df, train_size=0.05, stratify=df[\"label\"], random_state=42)\n",
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4242c2e",
   "metadata": {},
   "source": [
    "Convert to Hugging Face Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "519d8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46392cee",
   "metadata": {},
   "source": [
    "We define a function that uses the student tokenizer (i.e., DistilBERT) to tokenize our input texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c5b7e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_student(example):\n",
    "    return student_tokenizer(example[\"question_content\"], truncation=True, padding=\"max_length\", max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bbab15",
   "metadata": {},
   "source": [
    "We apply the tokenizer to our full dataset and split it into 80% training and 20% test data (with fixed seed, so everzone can reproduce it). And we format it as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502a091b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2969206b5e14a659252c2df4d4a8f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize_student, batched=True)\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc2b74",
   "metadata": {},
   "source": [
    "We are creating an **evaluation function** so we can evaluate the models on the flow, on the sane test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c91a9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_models(models_dir=\"models\", cache_file=\"model_eval_cache.json\", dataset=None, device=\"cuda\"):\n",
    "    if dataset is None:\n",
    "        raise ValueError(\"You must pass a dataset dictionary with a 'test' split.\")\n",
    "\n",
    "    # Load existing results:\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\") as f:\n",
    "            cache = json.load(f)\n",
    "    else:\n",
    "        cache = {}\n",
    "\n",
    "    test_loader = DataLoader(dataset[\"test\"], batch_size=64)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model_name in tqdm(sorted(os.listdir(models_dir))):\n",
    "        model_path = os.path.join(models_dir, model_name)\n",
    "\n",
    "        if not os.path.isdir(model_path):\n",
    "            continue\n",
    "\n",
    "        if model_name in cache:\n",
    "            cached = cache[model_name]\n",
    "            print(f\"Using cached results for {model_name}\")\n",
    "            print(f\"Accuracy: {cached['accuracy']:.4f}\")\n",
    "            print(f\"Precision: {cached['precision']:.4f}\")\n",
    "            print(f\"Recall: {cached['recall']:.4f}\")\n",
    "            results.append(cached)\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nEvaluating {model_name}\")\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "        model.eval()\n",
    "\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"label\"].cpu().numpy()\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, axis=1).cpu().numpy()\n",
    "\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels)\n",
    "\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        prec = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "        rec = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"Precision: {prec:.4f}\")\n",
    "        print(f\"Recall:    {rec:.4f}\")\n",
    "\n",
    "        result = {\n",
    "            \"version\": model_name,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec\n",
    "        }\n",
    "\n",
    "        # Save to cache and results:\n",
    "        cache[model_name] = result\n",
    "        results.append(result)\n",
    "\n",
    "        # We save after every model:\n",
    "        with open(cache_file, \"w\") as f:\n",
    "            json.dump(cache, f, indent=2)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25bb65",
   "metadata": {},
   "source": [
    "### **3. Distillation Training**\n",
    "\n",
    "We batch the training data into mini-batches of 32 and shuffle them each epoch to reduce overfitting.\n",
    "\n",
    "We use a simple loss function that encourages the student to match the teacher's predictions, this is AdamW, the standard optimizer (we had also used before).\n",
    "\n",
    "(Note that runtime we will compare after we have selected the best couple distilled models that have been trained for ,ore than 1 epoch etc., what we are doing right now is first to find good configurations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa854e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher_model.to(device).eval()\n",
    "student_model.to(device).train()\n",
    "\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=32, shuffle=True)\n",
    "optimizer = AdamW(student_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7bbaf7",
   "metadata": {},
   "source": [
    "This is for the validation (when we do earlzy stopping) to avoid overfitting we can evaluate on the test set (here we use batch size 64):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b4b535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(dataset[\"test\"], batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc25e02",
   "metadata": {},
   "source": [
    "We define a distillation loss that combines **cross-entropy loss** and **Kullback-Leibler divergence**.\n",
    "\n",
    "The paramters that we define here are:\n",
    "\n",
    "**temperature**: softens probabilities, making them more informative, we set it at 2, with this we have smoother distributions as we have see in lecture 9 (slide 18). This means we have more randomness in LLMs, here it means that the model is less canfidence in the distillation. So the student model can learn fine grained-relationships between classes.\n",
    "\n",
    "**alpha**: controls trade-off between matching true labels (CE) and teacher behavior (KL), so how much weight we give to the ground truth vs. the teacher. So with 0.5 we \"equally\" balance between the true labels and the teacher knowledge (via the KL divergence on the softened logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ceac390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student_logits, teacher_logits, true_labels, alpha=0.5, temperature=2.0):\n",
    "    ce_loss = F.cross_entropy(student_logits, true_labels)\n",
    "    kl_loss = F.kl_div(\n",
    "        F.log_softmax(student_logits / temperature, dim=1),\n",
    "        F.softmax(teacher_logits / temperature, dim=1),\n",
    "        reduction=\"batchmean\"\n",
    "    ) * (temperature ** 2)\n",
    "    return alpha * ce_loss + (1 - alpha) * kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77905ed4",
   "metadata": {},
   "source": [
    "#### *First Version of the Student Model*\n",
    "\n",
    "Just with three epochs and temperature = 2.0 and alpha 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23407c80",
   "metadata": {},
   "source": [
    "**Training Loop (Distillation):**\n",
    "\n",
    "We are using real teacher probabilities, not just argmax. With argmax, we would be doing hard predictions like [0,0,1,0,0,1,0,...]. But we are using soft distribution over classes, because they can carry moreinformation as hard labels. Obviouslz, if you consider distributions and when outcomes are Bernoulli, that is the least informative ditribution, you only know for example that class 2 is correct, but zou do not have anz information about the others. If you have soft targets (probability distribution) from the teacher, e.g., [0.01, 0.05, 0.70, 0.23, 0.01, ..., 0.00], then you learn that the teacher is mostly confident in class 2, but also thinks class 3 is plausible (e.g., it relates to computers),and class 1 is mildly relevant (math-related learning). The student now gets more nuanced supervision - not just \"what’s correct,\" but also \"how close are other classes?\" So what we get is a bigger or richer signal than we would get from the true label alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aee8fd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1370.8307\n",
      "Epoch 2: Loss = 1370.8407\n",
      "Epoch 3: Loss = 1370.6875\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        student_logits = student_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        loss = distillation_loss(student_logits, teacher_logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8197118d",
   "metadata": {},
   "source": [
    "This took 14 minutes and 27.1 seconds on a computer with GPU (NVIDIA GeForce RTX 4060 8GB GDDR6.)\n",
    "\n",
    "We can see that the student model is not really improving much, so it has already learned from both the teacher's predictions and the true labels (so more epochs may not lead to better perfromance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc1a4b",
   "metadata": {},
   "source": [
    "Save student model (first version):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a6dfe44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/student_v1/tokenizer_config.json',\n",
       " 'models/student_v1/special_tokens_map.json',\n",
       " 'models/student_v1/vocab.txt',\n",
       " 'models/student_v1/added_tokens.json',\n",
       " 'models/student_v1/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"models/student_v1\"\n",
    "student_model.save_pretrained(save_path)\n",
    "student_tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d97b5b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5349.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached results for student_v1\n",
      "Accuracy: 0.8272\n",
      "Precision: 0.8281\n",
      "Recall: 0.8238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_all_models(\n",
    "    models_dir=\"models\",\n",
    "    cache_file=\"model_eval_cache.json\",\n",
    "    dataset=dataset,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a43d4",
   "metadata": {},
   "source": [
    "We can see that the perfromance is already quite good for the first very simple version, still, we want to try to further improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c45b0",
   "metadata": {},
   "source": [
    "#### *Diverse Versions of the Student Model*\n",
    "\n",
    "Before we do further exploration of the student with different attention heads or hidden layers, or even longer training, using more epochs, we will first inspect different distillation parameters:\n",
    "\n",
    "**temperature**: 1.0, 2.0, 5.0\n",
    "\n",
    "Higher = softer logits = more signal in probabilities.\n",
    "\n",
    "**alpha**: 0.2, 0.5, 0.8\n",
    "\n",
    "Give more weight to matching the teacher.\n",
    "\n",
    "We still include the initial setup, since we now only want to do it with one epoch, just to see which may be better parameters (so it is comparable). We are also using batch size 64 for faster evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f887bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training student model with alpha=0.2, temperature=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done for alpha=0.2, temperature=1.0 | Loss: 417.8267\n",
      "\n",
      "Training student model with alpha=0.2, temperature=2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done for alpha=0.2, temperature=2.0 | Loss: 678.6311\n",
      "\n",
      "Training student model with alpha=0.2, temperature=5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done for alpha=0.2, temperature=5.0 | Loss: 602.1761\n",
      "\n",
      "Training student model with alpha=0.5, temperature=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done for alpha=0.5, temperature=1.0 | Loss: 487.7130\n",
      "\n",
      "Training student model with alpha=0.5, temperature=2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done for alpha=0.5, temperature=2.0 | Loss: 658.4089\n",
      "\n",
      "Training student model with alpha=0.5, temperature=5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done for alpha=0.5, temperature=5.0 | Loss: 619.1440\n",
      "\n",
      "Training student model with alpha=0.8, temperature=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done for alpha=0.8, temperature=1.0 | Loss: 556.1922\n",
      "\n",
      "Training student model with alpha=0.8, temperature=2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done for alpha=0.8, temperature=2.0 | Loss: 631.4859\n",
      "\n",
      "Training student model with alpha=0.8, temperature=5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done for alpha=0.8, temperature=5.0 | Loss: 613.5604\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset[\"train\"], batch_size=64, shuffle=True)\n",
    "\n",
    "# Parameter grid\n",
    "alphas = [0.2, 0.5, 0.8]\n",
    "temperatures = [1.0, 2.0, 5.0]\n",
    "\n",
    "for alpha in alphas:\n",
    "    for temperature in temperatures:\n",
    "        print(f\"\\nTraining student model with alpha={alpha}, temperature={temperature}\")\n",
    "\n",
    "        # Fresh student model each time\n",
    "        student_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            \"distilbert-base-uncased\", num_labels=10\n",
    "        ).to(device)\n",
    "        optimizer = AdamW(student_model.parameters(), lr=5e-5)\n",
    "        student_model.train()\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "            student_logits = student_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            loss = distillation_loss(student_logits, teacher_logits, labels, alpha=alpha, temperature=temperature)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch 1 done for alpha={alpha}, temperature={temperature} | Loss: {total_loss:.4f}\")\n",
    "\n",
    "        # Save model and tokenizer\n",
    "        save_path = f\"models/student_a{alpha}_t{temperature}/\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        student_model.save_pretrained(save_path)\n",
    "        student_tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c561f43",
   "metadata": {},
   "source": [
    "**Evaluate the Performance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49225223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating student_a0.2_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:14<02:14, 14.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6670\n",
      "Precision: 0.6679\n",
      "Recall:    0.6626\n",
      "\n",
      "Evaluating student_a0.2_t2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:30<02:00, 15.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6651\n",
      "Precision: 0.6663\n",
      "Recall:    0.6623\n",
      "\n",
      "Evaluating student_a0.2_t5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:45<01:45, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6665\n",
      "Precision: 0.6670\n",
      "Recall:    0.6634\n",
      "\n",
      "Evaluating student_a0.5_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:00<01:30, 15.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6651\n",
      "Precision: 0.6661\n",
      "Recall:    0.6614\n",
      "\n",
      "Evaluating student_a0.5_t2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:15<01:15, 15.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6599\n",
      "Precision: 0.6679\n",
      "Recall:    0.6538\n",
      "\n",
      "Evaluating student_a0.5_t5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:30<01:00, 15.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6651\n",
      "Precision: 0.6724\n",
      "Recall:    0.6625\n",
      "\n",
      "Evaluating student_a0.8_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:45<00:45, 15.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6604\n",
      "Precision: 0.6618\n",
      "Recall:    0.6582\n",
      "\n",
      "Evaluating student_a0.8_t2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:00<00:30, 15.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6581\n",
      "Precision: 0.6698\n",
      "Recall:    0.6516\n",
      "\n",
      "Evaluating student_a0.8_t5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:15<00:00, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6659\n",
      "Precision: 0.6607\n",
      "Recall:    0.6637\n",
      "Using cached results for student_v1\n",
      "Accuracy: 0.8272\n",
      "Precision: 0.8281\n",
      "Recall: 0.8238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_all_models(\n",
    "    models_dir=\"models\",\n",
    "    cache_file=\"model_eval_cache.json\",\n",
    "    dataset=dataset,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcacec32",
   "metadata": {},
   "source": [
    "From this we can see that the combination of alpha = 0.2 and temperature = 1.0 yielded the best results in terms of overall performance. It achieved the lowest training loss (417.83) and the highest evaluation accuracy (66.70%), along with strong precision and recall values. This means that our student model benefitted most from a stronger emphasis on the teacher's predictions (1 - alpha = 0.8) and sharper (less softened) output distributions from the teacher (temperature = 1.0).\n",
    "\n",
    "When increasing the temperature to 2.0 or 5.0, we observe higher training loss and slightly lower accuracy across all alpha values. This suggests that over-smoothing the teacher's output distribution (via higher temperatures) made the learning signal less precise, especially for a student trained on only one epoch and a limited dataset.\n",
    "\n",
    "Similarly, increasing alpha, which shifts the loss more toward the ground truth labels, did not yield better performance. For example, with alpha = 0.8, the models performed consistently worse across all temperatures. This indicates that the teacher’s knowledge was more informative than the sparse ground truth in this few-shot setting, and leveraging that knowledge more (via a lower alpha) was beneficial.\n",
    "\n",
    "So this tells us that for fast, low-resource distillation, it's best to rely more on the teacher predictions (meaning low alpha), and also to keep the teacher outputs sharp (low temperature), and limit over-smoothing and underfitting by avoiding overly softened distributions or too much reliance on hard labels.\n",
    "\n",
    "For all future experiments, we therefore fix the distillation parameters to **alpha = 0.2 and temperature = 1.0**.\n",
    "\n",
    "Note that of course we see that with the smaller batch size we had better perfromance, but now for tuning paprameters, we will stick with 64, and then later reduce it for the model with the best parameters again to 32 and then train it with more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309ec05",
   "metadata": {},
   "source": [
    "#### *Different Hidden Layer and Dimension Versions of a Non-Student Model*\n",
    "\n",
    "Now,  we evaluate variants of the student model without using pretrained weights, since we are deviating from the default DistilBERT architecture (i.e., number of layers and hidden size). These models are initialized from scratch and trained using only the supervision provided by the teacher through distillation.\n",
    "\n",
    "Full BERT consists of 12 layers, while DistilBERT reduces this to 6 layers. Here, we experiment with 4 layers, to further reduce inference cost, and 8 layers, which increases capacity and may enable the student to better approximate the teacher’s behavior during distillation, despite not starting from pretrained knowledge.\n",
    "\n",
    "\n",
    "In addition to the number of layers, we vary the hidden dimension, which determines the size of the embeddings and the hidden states within each transformer block. DistilBERT uses a hidden size of 768 by default. We test 768, the standard width, and 384, a smaller variant that significantly reduces the number of parameters and computational cost.\n",
    "\n",
    "We are also testing for different hidden dimensions. The hidden dimension defines the size of the embeddings and hidden states within each transformer block. DistilBERT uses a hidden size of 768 by default. We test the following two configurations: 768 (standard width), and 384, a smaller variant that significantly reduces the number of parameters and compute.\n",
    "\n",
    "\n",
    "For each configuration the feedforward (intermediate) dimension is set to four times the hidden size and the number of attention heads is chosen to ensure hidden_size % num_heads = 0.\n",
    "\n",
    "This setup allows us to explore how well different randomly initialized architectures can learn from a strong teacher via distillation, and how size impacts performance when not relying on pretrained knowledge.\n",
    "\n",
    "Best parameters from previous distillation sweep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ee4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "temperature = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338c9c4f",
   "metadata": {},
   "source": [
    "Training with this new hidden layer and dimensions configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57eb9759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training student with 4 layers and dim=384\n",
      "Epoch 1 complete | Layers: 4, Dim: 384 | Loss: 861.1549\n",
      "\n",
      "Training student with 4 layers and dim=768\n",
      "Epoch 1 complete | Layers: 4, Dim: 768 | Loss: 781.6780\n",
      "\n",
      "Training student with 6 layers and dim=384\n",
      "Epoch 1 complete | Layers: 6, Dim: 384 | Loss: 856.0326\n",
      "\n",
      "Training student with 6 layers and dim=768\n",
      "Epoch 1 complete | Layers: 6, Dim: 768 | Loss: 792.1129\n",
      "\n",
      "Training student with 8 layers and dim=384\n",
      "Epoch 1 complete | Layers: 8, Dim: 384 | Loss: 846.5542\n",
      "\n",
      "Training student with 8 layers and dim=768\n",
      "Epoch 1 complete | Layers: 8, Dim: 768 | Loss: 796.0934\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_options = [4, 6, 8]\n",
    "hidden_dim_options = [384, 768]\n",
    "\n",
    "for num_layers in hidden_layer_options:\n",
    "    for dim in hidden_dim_options:\n",
    "        print(f\"\\nTraining student with {num_layers} layers and dim={dim}\")\n",
    "\n",
    "        # Defining the model configuration:\n",
    "        config = DistilBertConfig(\n",
    "            num_labels=10,\n",
    "            n_layers=num_layers,\n",
    "            dim=dim, # embedding and hidden state size\n",
    "            hidden_dim=dim * 4,# feedforward size (standard)\n",
    "            n_heads=dim // 64, # ensure heads divide dim (e.g., 6 for dim=384)\n",
    "        )\n",
    "        student_model = DistilBertForSequenceClassification(config).to(device)\n",
    "        optimizer = AdamW(student_model.parameters(), lr=5e-5)\n",
    "        student_model.train()\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "            student_logits = student_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            loss = distillation_loss(student_logits, teacher_logits, labels, alpha=alpha, temperature=temperature)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch 1 complete | Layers: {num_layers}, Dim: {dim} | Loss: {total_loss:.4f}\")\n",
    "\n",
    "        # Save model\n",
    "        save_path = f\"models/student_l{num_layers}_d{dim}_a{alpha}_t{temperature}/\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        student_model.save_pretrained(save_path)\n",
    "        student_tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb579e8",
   "metadata": {},
   "source": [
    "**Evaluate the Performance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bcb6706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached results for student_a0.2_t1.0\n",
      "Accuracy: 0.6670\n",
      "Precision: 0.6679\n",
      "Recall: 0.6626\n",
      "Using cached results for student_a0.2_t2.0\n",
      "Accuracy: 0.6651\n",
      "Precision: 0.6663\n",
      "Recall: 0.6623\n",
      "Using cached results for student_a0.2_t5.0\n",
      "Accuracy: 0.6665\n",
      "Precision: 0.6670\n",
      "Recall: 0.6634\n",
      "Using cached results for student_a0.5_t1.0\n",
      "Accuracy: 0.6651\n",
      "Precision: 0.6661\n",
      "Recall: 0.6614\n",
      "Using cached results for student_a0.5_t2.0\n",
      "Accuracy: 0.6599\n",
      "Precision: 0.6679\n",
      "Recall: 0.6538\n",
      "Using cached results for student_a0.5_t5.0\n",
      "Accuracy: 0.6651\n",
      "Precision: 0.6724\n",
      "Recall: 0.6625\n",
      "Using cached results for student_a0.8_t1.0\n",
      "Accuracy: 0.6604\n",
      "Precision: 0.6618\n",
      "Recall: 0.6582\n",
      "Using cached results for student_a0.8_t2.0\n",
      "Accuracy: 0.6581\n",
      "Precision: 0.6698\n",
      "Recall: 0.6516\n",
      "Using cached results for student_a0.8_t5.0\n",
      "Accuracy: 0.6659\n",
      "Precision: 0.6607\n",
      "Recall: 0.6637\n",
      "\n",
      "Evaluating student_l4_d384_a0.2_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 10/16 [00:03<00:01,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4001\n",
      "Precision: 0.3538\n",
      "Recall:    0.3860\n",
      "\n",
      "Evaluating student_l4_d768_a0.2_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 11/16 [00:13<00:07,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4884\n",
      "Precision: 0.4933\n",
      "Recall:    0.4779\n",
      "\n",
      "Evaluating student_l6_d384_a0.2_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 12/16 [00:17<00:07,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3715\n",
      "Precision: 0.3625\n",
      "Recall:    0.3626\n",
      "\n",
      "Evaluating student_l6_d768_a0.2_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 13/16 [00:32<00:12,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4841\n",
      "Precision: 0.4862\n",
      "Recall:    0.4766\n",
      "\n",
      "Evaluating student_l8_d384_a0.2_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 14/16 [00:38<00:09,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3802\n",
      "Precision: 0.3601\n",
      "Recall:    0.3719\n",
      "\n",
      "Evaluating student_l8_d768_a0.2_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:58<00:00,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4630\n",
      "Precision: 0.5029\n",
      "Recall:    0.4505\n",
      "Using cached results for student_v1\n",
      "Accuracy: 0.8272\n",
      "Precision: 0.8281\n",
      "Recall: 0.8238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_all_models(\n",
    "    models_dir=\"models\",\n",
    "    cache_file=\"model_eval_cache.json\",\n",
    "    dataset=dataset,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242bd747",
   "metadata": {},
   "source": [
    "From these results, we can clearly observe a substantial performance difference between student models initialized with pretrained weights and those trained from scratch.\n",
    "\n",
    "The first block (students labeled with student_a...) shows results from models that retain the standard DistilBERT architecture (6 layers, 768 hidden dim) and are initialized with pretrained weights. These models reach an accuracy around 66.5%-66.7%, indicating that the knowledge distilled from the teacher is effectively guiding the student. The performance remains quite stable across different values of alpha and temperature, showing robustness of the distillation setup when using a well-initialized student.\n",
    "\n",
    "In contrast, the second block (students labeled student_l...) shows models with custom architectures (4/6/8 layers, 384/768 hidden size) that were initialized from scratch. These models perform significantly worse, with accuracies ranging from 37% to 49%. Even the largest variant tested (l4_d768 or l6_d768) fails to reach the baseline set by the pretrained student models. The worst performing variants (e.g., l6_d384, l8_d384) show that simply adding layers or reducing hidden size, without the benefit of pretrained weights, leads to unstable and ineffective training under distillation.\n",
    "\n",
    "This highlights the importance of pretrained initialization, especially when using relatively shallow students. While training from scratch may offer flexibility in architectural choices, it also makes optimization much harder, especially in low-data or one-epoch distillation settings like this.\n",
    "\n",
    "As we had expected, pretrained student models with default architecture perform significantly better under distillation. The custom student models trained from scratch suffer from poor accuracy and cannot leverage the teacher’s knowledge effectively.\n",
    "\n",
    "However, if architectural changes are desired (e.g., fewer layers or smaller dimensions), it may be crucial to pretrain these models first (e.g., via masked language modeling) before distillation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c302bc1",
   "metadata": {},
   "source": [
    "#### *Different Learning Rate Versions of the Student Model*\n",
    "\n",
    "Next, we investigate how varying the learning rate impacts the performance of the student model. The learning rate controls how much the model updates its weights during training and is often one of the most sensitive hyperparameters in optimization.\n",
    "\n",
    "Previously, we fixed the learning rate at `5e-5`. While this is commonly used for BERT-like models, smaller student architectures may benefit from **slightly larger** learning rates to compensate for reduced capacity and encourage faster adaptation.\n",
    "\n",
    "We evaluate the following learning rates:\n",
    "\n",
    "- **3e-5**: A conservative value often used for full fine-tuning with large models.\n",
    "- **5e-5**: The baseline used in all previous experiments.\n",
    "- **1e-4**: A more aggressive value that can help smaller models converge faster, especially during short (1-epoch) training.\n",
    "\n",
    "Each student model uses the best distillation parameters (`alpha = 0.2`, `temperature = 1.0`) and is trained for one epoch, as before. We fix the architecture using the default number of layers and dimensions so we can use the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7adefcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training student with lr=3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete | LR: 3e-05 | Loss: 432.5083\n",
      "\n",
      "Training student with lr=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete | LR: 5e-05 | Loss: 420.8752\n",
      "\n",
      "Training student with lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete | LR: 1e-04 | Loss: 409.9021\n"
     ]
    }
   ],
   "source": [
    "# Learning rates to evaluate\n",
    "learning_rates = [3e-5, 5e-5, 1e-4]\n",
    "\n",
    "# Best distillation params\n",
    "alpha = 0.2\n",
    "temperature = 1.0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTraining student with lr={lr}\")\n",
    "\n",
    "    # Load pretrained DistilBERT with classification head\n",
    "    student_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", num_labels=10\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(student_model.parameters(), lr=lr)\n",
    "    student_model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        student_logits = student_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        loss = distillation_loss(student_logits, teacher_logits, labels, alpha=alpha, temperature=temperature)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch 1 complete | LR: {lr:.0e} | Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    save_path = f\"models/student_pretrained_lr{lr:.0e}_a{alpha}_t{temperature}/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    student_model.save_pretrained(save_path)\n",
    "    student_tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e8d3e",
   "metadata": {},
   "source": [
    "**Evaluate the Performance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3599c9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached results for student_a0.2_t1.0\n",
      "Accuracy: 0.6670\n",
      "Precision: 0.6679\n",
      "Recall: 0.6626\n",
      "Using cached results for student_a0.2_t2.0\n",
      "Accuracy: 0.6651\n",
      "Precision: 0.6663\n",
      "Recall: 0.6623\n",
      "Using cached results for student_a0.2_t5.0\n",
      "Accuracy: 0.6665\n",
      "Precision: 0.6670\n",
      "Recall: 0.6634\n",
      "Using cached results for student_a0.5_t1.0\n",
      "Accuracy: 0.6651\n",
      "Precision: 0.6661\n",
      "Recall: 0.6614\n",
      "Using cached results for student_a0.5_t2.0\n",
      "Accuracy: 0.6599\n",
      "Precision: 0.6679\n",
      "Recall: 0.6538\n",
      "Using cached results for student_a0.5_t5.0\n",
      "Accuracy: 0.6651\n",
      "Precision: 0.6724\n",
      "Recall: 0.6625\n",
      "Using cached results for student_a0.8_t1.0\n",
      "Accuracy: 0.6604\n",
      "Precision: 0.6618\n",
      "Recall: 0.6582\n",
      "Using cached results for student_a0.8_t2.0\n",
      "Accuracy: 0.6581\n",
      "Precision: 0.6698\n",
      "Recall: 0.6516\n",
      "Using cached results for student_a0.8_t5.0\n",
      "Accuracy: 0.6659\n",
      "Precision: 0.6607\n",
      "Recall: 0.6637\n",
      "Using cached results for student_l4_d384_a0.2_t1.0\n",
      "Accuracy: 0.4001\n",
      "Precision: 0.3538\n",
      "Recall: 0.3860\n",
      "Using cached results for student_l4_d768_a0.2_t1.0\n",
      "Accuracy: 0.4884\n",
      "Precision: 0.4933\n",
      "Recall: 0.4779\n",
      "Using cached results for student_l6_d384_a0.2_t1.0\n",
      "Accuracy: 0.3715\n",
      "Precision: 0.3625\n",
      "Recall: 0.3626\n",
      "Using cached results for student_l6_d768_a0.2_t1.0\n",
      "Accuracy: 0.4841\n",
      "Precision: 0.4862\n",
      "Recall: 0.4766\n",
      "Using cached results for student_l8_d384_a0.2_t1.0\n",
      "Accuracy: 0.3802\n",
      "Precision: 0.3601\n",
      "Recall: 0.3719\n",
      "Using cached results for student_l8_d768_a0.2_t1.0\n",
      "Accuracy: 0.4630\n",
      "Precision: 0.5029\n",
      "Recall: 0.4505\n",
      "\n",
      "Evaluating student_pretrained_lr1e-04_a0.2_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 16/19 [00:15<00:02,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6609\n",
      "Precision: 0.6671\n",
      "Recall:    0.6575\n",
      "\n",
      "Evaluating student_pretrained_lr3e-05_a0.2_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 17/19 [00:30<00:04,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6636\n",
      "Precision: 0.6643\n",
      "Recall:    0.6594\n",
      "\n",
      "Evaluating student_pretrained_lr5e-05_a0.2_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:45<00:00,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6588\n",
      "Precision: 0.6591\n",
      "Recall:    0.6538\n",
      "Using cached results for student_v1\n",
      "Accuracy: 0.8272\n",
      "Precision: 0.8281\n",
      "Recall: 0.8238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_all_models(\n",
    "    models_dir=\"models\",\n",
    "    cache_file=\"model_eval_cache.json\",\n",
    "    dataset=dataset,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d17e64",
   "metadata": {},
   "source": [
    "From this we can see that varying the learning rate has a noticeable effect on both the training loss and the final evaluation metrics, even when using the same pretrained DistilBERT architecture and fixed distillation parameters (alpha = 0.2, temp. = 1.0).\n",
    "\n",
    "Based on the training loss after one epoch we can see that it steadily decreases as the learning rate increases, this suggests that higher learning rates help the student model converge faster, likely because the pretrained base allows for more aggressive updates without diverging.\n",
    "\n",
    "The evaluation performance, however, does not strictly follow the same trend. In terms of accuracy, the 3e-5 was the the best performing configuration. \n",
    "\n",
    "From this we can conclude that while 1e-4 led to the lowest training loss, it did not generalize as well as the more conservative 3e-5. This confirms that lower learning rates tend to stabilize training and improve generalization, especially when only training for a single epoch. At the same time, the differences are relatively small, and all three configurations outperformed the student models trained from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca16757e",
   "metadata": {},
   "source": [
    "#### *Intermediate Layer Distillation Version of the Student Model*\n",
    "\n",
    "Next, we extend the standard knowledge distillation setup by adding **intermediate layer distillation**, inspired by the TinyBERT framework.\n",
    "\n",
    "Instead of only matching the teacher and student logits at the output layer, we now also align the **hidden states** from intermediate transformer layers. This approach enables the student to learn internal representations that are more closely aligned with the teacher, potentially improving generalization and convergence.\n",
    "\n",
    "We use the pretrained `distilbert-base-uncased` student model and freeze the teacher. We extract hidden states from both models and define a **combined distillation loss** that includes:\n",
    "\n",
    "- **Soft target loss**: KL divergence between teacher and student logits (as before)\n",
    "- **Intermediate hidden loss**: Mean squared error between selected hidden layers\n",
    "\n",
    "We know that the student has fewer layers than the teacher (6), and map corresponding layers using a simple linear strategy (student layers match every 2nd layer of the 12-layer BERT teacher).\n",
    "Again, if we change the number of layers here, we break the mapping that every second layer of the DistilBERT nicely aligns with the teacher (BERT).\n",
    "\n",
    "\n",
    "\n",
    "We normalize the intermediate loss by the number of matched layers. Doing that ensures that the scale of the intermediate loss stays roughly independent of the number of matched layers, but it also leads to that the contribution of intermediate supervision is consistent and interpretable. Another point why we do it here, which may be most important is that we can re-use the same loss weight across experiments without needing to adjust for each model depth.\n",
    "\n",
    "The following function *combines soft label distillation with intermediate hidden state loss (somewhat like TinyBERT-style):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0df51c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_distillation_loss(student_outputs, teacher_outputs, student_logits, teacher_logits, labels, alpha, temperature):\n",
    "    # Soft target loss (as before)\n",
    "    soft_loss = F.kl_div(\n",
    "        F.log_softmax(student_logits / temperature, dim=-1),\n",
    "        F.softmax(teacher_logits / temperature, dim=-1),\n",
    "        reduction=\"batchmean\"\n",
    "    ) * (temperature ** 2)\n",
    "\n",
    "    # Supervised loss\n",
    "    hard_loss = F.cross_entropy(student_logits, labels)\n",
    "\n",
    "    # Intermediate loss between selected hidden states\n",
    "    student_hidden = student_outputs.hidden_states  # tuple of tensors\n",
    "    teacher_hidden = teacher_outputs.hidden_states\n",
    "\n",
    "    # Match layer i of student to layer 2i of teacher\n",
    "    inter_loss = 0.0\n",
    "    for i in range(len(student_hidden)):\n",
    "        inter_loss += F.mse_loss(student_hidden[i], teacher_hidden[2 * i])\n",
    "    inter_loss /= len(student_hidden)  # normalize\n",
    "\n",
    "    # Combine all\n",
    "    return alpha * soft_loss + (1 - alpha) * hard_loss + 0.1 * inter_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea997a77",
   "metadata": {},
   "source": [
    "**Training:**\n",
    "\n",
    "Since the intermediate distillation version we want to do builds on top of that same loss framework as the DistilBERT, we keep the alpha and temperature fixed, i.e., the best values that we found. \n",
    "\n",
    "However, the learning-rates should be re-evaluated since adding intermediate-layer distillation introduces new gradients and loss components (from hidden states) which changes the optimization, so there are more layers that may contribute to the total loss, the scale of the loss may increase and learning dznamics may increase.\n",
    "\n",
    "Thus, we again evaluate which is the best learning rate from the same set as before.\n",
    "\n",
    "The teacher stays frozen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c403408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training student with lr=3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete | LR: 3e-05 | Loss: 586.3482\n",
      "\n",
      "Training student with lr=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete | LR: 5e-05 | Loss: 569.2524\n",
      "\n",
      "Training student with lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete | LR: 1e-04 | Loss: 560.4650\n"
     ]
    }
   ],
   "source": [
    "# Teacher model outputs hidden states:\n",
    "teacher_model.config.output_hidden_states = True\n",
    "\n",
    "# Training params:\n",
    "learning_rates = [3e-5, 5e-5, 1e-4]\n",
    "alpha = 0.2\n",
    "temperature = 1.0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTraining student with lr={lr}\")\n",
    "\n",
    "    # Load pretrained DistilBERT with classification head and hidden states:\n",
    "    student_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", num_labels=10,\n",
    "        output_hidden_states=True\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(student_model.parameters(), lr=lr)\n",
    "    student_model.train()\n",
    "    teacher_model.eval()  # Teacher stays frozen\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Get teacher outputs with hidden states\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "\n",
    "        # Get student outputs with hidden states:\n",
    "        student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        # Our combined loss:\n",
    "        loss = intermediate_distillation_loss(\n",
    "            student_outputs, teacher_outputs,\n",
    "            student_logits, teacher_logits,\n",
    "            labels, alpha, temperature\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch 1 complete | LR: {lr:.0e} | Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Save the model:\n",
    "    save_path = f\"models/student_intermediate_lr{lr:.0e}_a{alpha}_t{temperature}/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    student_model.save_pretrained(save_path)\n",
    "    student_tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c91bc9",
   "metadata": {},
   "source": [
    "**Evaluate the Performance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48750c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached results for student_a0.2_t1.0\n",
      "Accuracy: 0.6670\n",
      "Precision: 0.6679\n",
      "Recall: 0.6626\n",
      "Using cached results for student_a0.2_t2.0\n",
      "Accuracy: 0.6651\n",
      "Precision: 0.6663\n",
      "Recall: 0.6623\n",
      "Using cached results for student_a0.2_t5.0\n",
      "Accuracy: 0.6665\n",
      "Precision: 0.6670\n",
      "Recall: 0.6634\n",
      "Using cached results for student_a0.5_t1.0\n",
      "Accuracy: 0.6651\n",
      "Precision: 0.6661\n",
      "Recall: 0.6614\n",
      "Using cached results for student_a0.5_t2.0\n",
      "Accuracy: 0.6599\n",
      "Precision: 0.6679\n",
      "Recall: 0.6538\n",
      "Using cached results for student_a0.5_t5.0\n",
      "Accuracy: 0.6651\n",
      "Precision: 0.6724\n",
      "Recall: 0.6625\n",
      "Using cached results for student_a0.8_t1.0\n",
      "Accuracy: 0.6604\n",
      "Precision: 0.6618\n",
      "Recall: 0.6582\n",
      "Using cached results for student_a0.8_t2.0\n",
      "Accuracy: 0.6581\n",
      "Precision: 0.6698\n",
      "Recall: 0.6516\n",
      "Using cached results for student_a0.8_t5.0\n",
      "Accuracy: 0.6659\n",
      "Precision: 0.6607\n",
      "Recall: 0.6637\n",
      "\n",
      "Evaluating student_intermediate_lr1e-04_a0.2_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 10/22 [00:15<00:18,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6515\n",
      "Precision: 0.6511\n",
      "Recall:    0.6516\n",
      "\n",
      "Evaluating student_intermediate_lr3e-05_a0.2_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 11/22 [00:30<00:35,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6623\n",
      "Precision: 0.6627\n",
      "Recall:    0.6590\n",
      "\n",
      "Evaluating student_intermediate_lr5e-05_a0.2_t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:45<00:00,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6517\n",
      "Precision: 0.6539\n",
      "Recall:    0.6489\n",
      "Using cached results for student_l4_d384_a0.2_t1.0\n",
      "Accuracy: 0.4001\n",
      "Precision: 0.3538\n",
      "Recall: 0.3860\n",
      "Using cached results for student_l4_d768_a0.2_t1.0\n",
      "Accuracy: 0.4884\n",
      "Precision: 0.4933\n",
      "Recall: 0.4779\n",
      "Using cached results for student_l6_d384_a0.2_t1.0\n",
      "Accuracy: 0.3715\n",
      "Precision: 0.3625\n",
      "Recall: 0.3626\n",
      "Using cached results for student_l6_d768_a0.2_t1.0\n",
      "Accuracy: 0.4841\n",
      "Precision: 0.4862\n",
      "Recall: 0.4766\n",
      "Using cached results for student_l8_d384_a0.2_t1.0\n",
      "Accuracy: 0.3802\n",
      "Precision: 0.3601\n",
      "Recall: 0.3719\n",
      "Using cached results for student_l8_d768_a0.2_t1.0\n",
      "Accuracy: 0.4630\n",
      "Precision: 0.5029\n",
      "Recall: 0.4505\n",
      "Using cached results for student_pretrained_lr1e-04_a0.2_t1.0\n",
      "Accuracy: 0.6609\n",
      "Precision: 0.6671\n",
      "Recall: 0.6575\n",
      "Using cached results for student_pretrained_lr3e-05_a0.2_t1.0\n",
      "Accuracy: 0.6636\n",
      "Precision: 0.6643\n",
      "Recall: 0.6594\n",
      "Using cached results for student_pretrained_lr5e-05_a0.2_t1.0\n",
      "Accuracy: 0.6588\n",
      "Precision: 0.6591\n",
      "Recall: 0.6538\n",
      "Using cached results for student_v1\n",
      "Accuracy: 0.8272\n",
      "Precision: 0.8281\n",
      "Recall: 0.8238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_all_models(\n",
    "    models_dir=\"models\",\n",
    "    cache_file=\"model_eval_cache.json\",\n",
    "    dataset=dataset,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a01a96",
   "metadata": {},
   "source": [
    "From this we can see that training smaller student architectures from scratch, even when incorporating intermediate layer supervision, leads to lower performance compared to models that utilize pretrained weights. All models were initialized from the base pretrained checkpoint, with classification layers newly initialized.\n",
    "\n",
    "During training, the total loss decreased as the learning rate increased. At a learning rate of 3e-5, the model reached a loss of approximately 586.35. With 5e-5, the loss dropped to 569.25, and with 1e-4 it decreased further to 560.47. This pattern indicates that larger learning rates help the student model adapt more quickly, especially when layers such as the classification head are randomly initialized and only limited training is performed.\n",
    "\n",
    "However, when evaluating performance on the validation set, the relationship between learning rate and generalization was less straightforward. The model trained with the lowest learning rate, 3e-5, achieved the highest accuracy at 66.23%. The models trained with 5e-5 and 1e-4 achieved slightly lower accuracies of 65.17% and 65.15%, respectively. From this we can see that although larger learning rates reduce training loss, they do not necessarily improve validation accuracy, so the more conservative learning rate of 3e-5 appears to offer the best balance between learning and generalization in this setup.\n",
    "\n",
    "Compared to the TinyBERT-style scratch-trained models (from earlier) with reduced hidden size and fewer layers (for example 4 layers with 384 hidden dimensions), which struggled to exceed 49% accuracy, the intermediate distillation models, which preserve the full DistilBERT architecture and leverage both soft targets and hidden state alignment, achieved significantly better results, reaching over 66% accuracy. This demonstrates the strong benefit of combining pretrained knowledge with intermediate supervision, even when the training data and number of epochs are limited.\n",
    "\n",
    "Nevertheless, these models still slightly underperform relative to the best student models trained with standard distillation and fully pretrained weights. For instance, the best standard distillation model with a learning rate of 3e-5 reached 66.36% accuracy, outperforming all intermediate distillation variants by a small but consistent margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa1839",
   "metadata": {},
   "source": [
    "#### *Best Version of the Student Model*\n",
    "\n",
    "Now, we run our best models with maximum 20 epochs and early stopping, where we stop the training if it does not improve after 3 epochs. We also use a batch size of 32 now (and not 64). The models we include are:\n",
    "\n",
    "One version of the standard distillation (pretrained encoder), the *student_pretrained_lr3e-05_a0.2_t1.0*, since it has the best current performance, fewer moving parts, so we can see how much performance can improve over more epochs.\n",
    "\n",
    "And one version of the intermediate distillation (TinyBERT-style, pretrained encoder), the *student_intermediate_lr3e-05_a0.2_t1.0*, since it benefits from both soft targets and internal representation alignment, so we can test if its advantage increases over time and training steps.\n",
    "\n",
    "We first set the batch size to 32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0b98c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset[\"train\"], batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08950e",
   "metadata": {},
   "source": [
    "**Training loop:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f931b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_model(\n",
    "    student_model, \n",
    "    student_tokenizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    teacher_model,\n",
    "    optimizer,\n",
    "    loss_fn,  \n",
    "    model_name,  \n",
    "    alpha=0.2,\n",
    "    temperature=1.0,\n",
    "    patience=3,\n",
    "    max_epochs=20\n",
    "):\n",
    "    best_val_acc = 0\n",
    "    counter = 0\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        student_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                teacher_logits = teacher_outputs.logits\n",
    "\n",
    "            student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            student_logits = student_outputs.logits\n",
    "\n",
    "            # Handle standard vs. intermediate distillation loss:\n",
    "            if loss_fn.__name__ == \"distillation_loss\":\n",
    "                loss = loss_fn(student_logits, teacher_logits, labels, alpha, temperature)\n",
    "            else:\n",
    "                loss = loss_fn(\n",
    "                    student_outputs,\n",
    "                    teacher_outputs,\n",
    "                    student_logits,\n",
    "                    teacher_logits,\n",
    "                    labels,\n",
    "                    alpha,\n",
    "                    temperature\n",
    "                )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        student_model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"label\"].to(\"cpu\").numpy()\n",
    "\n",
    "                outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels)\n",
    "\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_accuracies.append(val_acc)\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "            print(\"New best model found. Saving...\")\n",
    "            save_path = f\"models/{model_name}\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            student_model.save_pretrained(save_path)\n",
    "            student_tokenizer.save_pretrained(save_path)\n",
    "        else:\n",
    "            counter += 1\n",
    "            print(f\"No improvement. Early stopping counter: {counter}/{patience}\")\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Loss curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, marker='o')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Training Loss (log scale)\")\n",
    "    plt.title(f\"Loss Curve: {model_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81613bde",
   "metadata": {},
   "source": [
    "**Load student models:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879a0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard distillation student (pretrained encoder)\n",
    "student_pretrained = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"models/student_pretrained_lr3e-05_a0.2_t1.0\"\n",
    ").to(device)\n",
    "\n",
    "# Intermediate distillation student (TinyBERT-style)\n",
    "student_intermediate = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"models/student_intermediate_lr3e-05_a0.2_t1.0\"\n",
    ").to(device)\n",
    "\n",
    "# Tokenizer (same for both)\n",
    "student_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5101cec",
   "metadata": {},
   "source": [
    "**Define optimizers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0bde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_pretrained = AdamW(student_pretrained.parameters(), lr=3e-5)\n",
    "optimizer_intermediate = AdamW(student_intermediate.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326e2fb",
   "metadata": {},
   "source": [
    "The loss functions are already defined, now we can **start the training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88bc186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5730\n",
      "Validation Accuracy: 0.6621\n",
      "New best model found. Saving...\n",
      "Epoch 2: Train Loss = 0.3898\n",
      "Validation Accuracy: 0.6550\n",
      "No improvement. Early stopping counter: 1/3\n",
      "Epoch 3: Train Loss = 0.2710\n",
      "Validation Accuracy: 0.6566\n",
      "No improvement. Early stopping counter: 2/3\n",
      "Epoch 4: Train Loss = 0.2049\n",
      "Validation Accuracy: 0.6654\n",
      "New best model found. Saving...\n",
      "Epoch 5: Train Loss = 0.1739\n",
      "Validation Accuracy: 0.6618\n",
      "No improvement. Early stopping counter: 1/3\n",
      "Epoch 6: Train Loss = 0.1574\n",
      "Validation Accuracy: 0.6637\n",
      "No improvement. Early stopping counter: 2/3\n",
      "Epoch 7: Train Loss = 0.1474\n",
      "Validation Accuracy: 0.6613\n",
      "No improvement. Early stopping counter: 3/3\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.8747\n",
      "Validation Accuracy: 0.6686\n",
      "New best model found. Saving...\n",
      "Epoch 2: Train Loss = 0.6649\n",
      "Validation Accuracy: 0.6545\n",
      "No improvement. Early stopping counter: 1/3\n",
      "Epoch 3: Train Loss = 0.4914\n",
      "Validation Accuracy: 0.6508\n",
      "No improvement. Early stopping counter: 2/3\n",
      "Epoch 4: Train Loss = 0.3655\n",
      "Validation Accuracy: 0.6380\n",
      "No improvement. Early stopping counter: 3/3\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard distillation\n",
    "train_student_model(\n",
    "    student_model=student_pretrained,\n",
    "    student_tokenizer=student_tokenizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    teacher_model=teacher_model,\n",
    "    optimizer=optimizer_pretrained,\n",
    "    loss_fn=distillation_loss,\n",
    "    model_name=\"student_best_pretrained\",\n",
    "    alpha=0.2,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "# Intermediate distillation\n",
    "train_student_model(\n",
    "    student_model=student_intermediate,\n",
    "    student_tokenizer=student_tokenizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    teacher_model=teacher_model,\n",
    "    optimizer=optimizer_intermediate,\n",
    "    loss_fn=intermediate_distillation_loss,\n",
    "    model_name=\"student_best_intermediate\",\n",
    "    alpha=0.2,\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ae194",
   "metadata": {},
   "source": [
    "**student_best_pretrained**\n",
    "\n",
    "The standard distilled student model, using a pretrained encoder and trained with soft targets only, shows a clear and steady decrease in training loss over 7 epochs. The log-scaled curve highlights that learning was stable, with diminishing returns over time.\n",
    "\n",
    "In Epoch 1, the model achieved a training loss of 0.5730 and a validation accuracy of 66.21%, setting the initial performance benchmark.\n",
    "\n",
    "While the training loss improved substantially in the next two epochs (0.3898 and 0.2710), validation accuracy slightly dropped to 65.50% and 65.66%, triggering early stopping counters.\n",
    "\n",
    "In Epoch 4, the model reached a new best validation accuracy of 66.54% as training loss further decreased to 0.2049.\n",
    "\n",
    "The following epochs continued to improve the training loss (down to 0.1474 in Epoch 7), but validation accuracy fluctuated slightly below the peak (66.18%, 66.37%, 66.13%).\n",
    "\n",
    "Since no further improvement occurred for three consecutive epochs, early stopping was triggered after Epoch 7.\n",
    "\n",
    "Overall, this model showed solid convergence, and the loss curve suggests that the model was still learning, albeit with reduced gains. The final training loss was low, and validation performance was competitive.\n",
    "\n",
    "**student_best_intermediate**\n",
    "\n",
    "This intermediate-distilled model (TinyBERT-style, pretrained encoder, with hidden-state matching) showed faster convergence in terms of loss reduction, but generalization performance did not follow the same trend.\n",
    "\n",
    "In Epoch 1, the training loss was higher at 0.8747, but the model surprisingly reached the highest validation accuracy of 66.86% right away.\n",
    "\n",
    "Subsequent epochs saw consistent reductions in training loss (0.6649, 0.4914, and 0.3655), but validation accuracy declined to 65.45%, 65.08%, and 63.80%.\n",
    "\n",
    "This drop in validation accuracy, despite better training loss, suggests the model may have overfit quickly or learned superficial patterns in early epochs.\n",
    "\n",
    "After no improvement for three epochs, early stopping was triggered at Epoch 4.\n",
    "\n",
    "The corresponding loss curve confirms the rapid drop in training loss, but the decoupling from validation performance highlights a generalization issue. The model's strong initial result might reflect a lucky initialization or effective transfer from the internal alignment objective, but it was not sustained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba67b15",
   "metadata": {},
   "source": [
    "**Evaluation:**\n",
    "\n",
    "First we load both saved models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b733835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained = DistilBertForSequenceClassification.from_pretrained(\"models/student_best_pretrained\").to(device)\n",
    "model_intermediate = DistilBertForSequenceClassification.from_pretrained(\"models/student_best_intermediate\").to(device)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"models/student_best_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87acc7f",
   "metadata": {},
   "source": [
    "Shorter evaluation function, that only shows us the perfromance of those two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7e8c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    val_loader = DataLoader(dataset[\"test\"], batch_size=64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device) if device != \"cpu\" else batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"].to(device) if device != \"cpu\" else batch[\"attention_mask\"]\n",
    "            labels = batch[\"label\"].to(\"cpu\").numpy()  # Always move labels to CPU before .numpy()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    return acc, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43afe270",
   "metadata": {},
   "source": [
    "Final scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7cc2c58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Standard] Accuracy: 0.6654, Precision: 0.6634, Recall: 0.6633\n",
      "[Intermediate] Accuracy: 0.6686, Precision: 0.6671, Recall: 0.6646\n"
     ]
    }
   ],
   "source": [
    "acc, prec, rec = evaluate(model_pretrained, dataset, device)\n",
    "print(f\"[Standard] Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}\")\n",
    "\n",
    "acc, prec, rec = evaluate(model_intermediate, dataset, device)\n",
    "print(f\"[Intermediate] Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b924f",
   "metadata": {},
   "source": [
    "The saved student_v1 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0357f69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V1 Model] Accuracy: 0.8272, Precision: 0.8281, Recall: 0.8238\n"
     ]
    }
   ],
   "source": [
    "# Load the saved student_v1 model\n",
    "model_path = \"models/student_v1\"\n",
    "student_v1 = DistilBertForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "\n",
    "# Evaluate using your new function\n",
    "acc, prec, rec = evaluate(student_v1, dataset, device)\n",
    "print(f\"[V1 Model] Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c3e5c",
   "metadata": {},
   "source": [
    "All results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3165d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached results for student_a0.2_t1.0\n",
      "Accuracy: 0.6670\n",
      "Precision: 0.6679\n",
      "Recall: 0.6626\n",
      "Using cached results for student_a0.2_t2.0\n",
      "Accuracy: 0.6651\n",
      "Precision: 0.6663\n",
      "Recall: 0.6623\n",
      "Using cached results for student_a0.2_t5.0\n",
      "Accuracy: 0.6665\n",
      "Precision: 0.6670\n",
      "Recall: 0.6634\n",
      "Using cached results for student_a0.5_t1.0\n",
      "Accuracy: 0.6651\n",
      "Precision: 0.6661\n",
      "Recall: 0.6614\n",
      "Using cached results for student_a0.5_t2.0\n",
      "Accuracy: 0.6599\n",
      "Precision: 0.6679\n",
      "Recall: 0.6538\n",
      "Using cached results for student_a0.5_t5.0\n",
      "Accuracy: 0.6651\n",
      "Precision: 0.6724\n",
      "Recall: 0.6625\n",
      "Using cached results for student_a0.8_t1.0\n",
      "Accuracy: 0.6604\n",
      "Precision: 0.6618\n",
      "Recall: 0.6582\n",
      "Using cached results for student_a0.8_t2.0\n",
      "Accuracy: 0.6581\n",
      "Precision: 0.6698\n",
      "Recall: 0.6516\n",
      "Using cached results for student_a0.8_t5.0\n",
      "Accuracy: 0.6659\n",
      "Precision: 0.6607\n",
      "Recall: 0.6637\n",
      "\n",
      "Evaluating student_best_intermediate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [00:14<00:22,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6686\n",
      "Precision: 0.6671\n",
      "Recall:    0.6646\n",
      "\n",
      "Evaluating student_best_pretrained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [00:29<00:43,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6654\n",
      "Precision: 0.6634\n",
      "Recall:    0.6633\n",
      "Using cached results for student_intermediate_lr1e-04_a0.2_t1.0\n",
      "Accuracy: 0.6515\n",
      "Precision: 0.6511\n",
      "Recall: 0.6516\n",
      "Using cached results for student_intermediate_lr3e-05_a0.2_t1.0\n",
      "Accuracy: 0.6623\n",
      "Precision: 0.6627\n",
      "Recall: 0.6590\n",
      "Using cached results for student_intermediate_lr5e-05_a0.2_t1.0\n",
      "Accuracy: 0.6517\n",
      "Precision: 0.6539\n",
      "Recall: 0.6489\n",
      "Using cached results for student_l4_d384_a0.2_t1.0\n",
      "Accuracy: 0.4001\n",
      "Precision: 0.3538\n",
      "Recall: 0.3860\n",
      "Using cached results for student_l4_d768_a0.2_t1.0\n",
      "Accuracy: 0.4884\n",
      "Precision: 0.4933\n",
      "Recall: 0.4779\n",
      "Using cached results for student_l6_d384_a0.2_t1.0\n",
      "Accuracy: 0.3715\n",
      "Precision: 0.3625\n",
      "Recall: 0.3626\n",
      "Using cached results for student_l6_d768_a0.2_t1.0\n",
      "Accuracy: 0.4841\n",
      "Precision: 0.4862\n",
      "Recall: 0.4766\n",
      "Using cached results for student_l8_d384_a0.2_t1.0\n",
      "Accuracy: 0.3802\n",
      "Precision: 0.3601\n",
      "Recall: 0.3719\n",
      "Using cached results for student_l8_d768_a0.2_t1.0\n",
      "Accuracy: 0.4630\n",
      "Precision: 0.5029\n",
      "Recall: 0.4505\n",
      "Using cached results for student_pretrained_lr1e-04_a0.2_t1.0\n",
      "Accuracy: 0.6609\n",
      "Precision: 0.6671\n",
      "Recall: 0.6575\n",
      "Using cached results for student_pretrained_lr3e-05_a0.2_t1.0\n",
      "Accuracy: 0.6636\n",
      "Precision: 0.6643\n",
      "Recall: 0.6594\n",
      "Using cached results for student_pretrained_lr5e-05_a0.2_t1.0\n",
      "Accuracy: 0.6588\n",
      "Precision: 0.6591\n",
      "Recall: 0.6538\n",
      "Using cached results for student_v1\n",
      "Accuracy: 0.8272\n",
      "Precision: 0.8281\n",
      "Recall: 0.8238\n",
      "\n",
      "Evaluating student_worst_intermediate_long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:44<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6546\n",
      "Precision: 0.6586\n",
      "Recall:    0.6517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_all_models(\n",
    "    models_dir=\"models\",\n",
    "    cache_file=\"model_eval_cache.json\",\n",
    "    dataset=dataset,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d6046",
   "metadata": {},
   "source": [
    "Interpretation will be in part b, since there we will compare them to the teacher directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93f9e0",
   "metadata": {},
   "source": [
    "### **4. Dynamic Quantization**\n",
    "\n",
    "Quantization is a technique used in model compression that reduces the precision of the numbers used to represent model parameters (e.g., weights) and sometimes activations.\n",
    "\n",
    "Deep learning models typically use 32-bit floating point (FP32) numbers. Quantization reduces this to 8-bit integers (INT8) or 16-bit floats, depending on the method (we use 8).\n",
    "\n",
    "Dynamic quantization specifically only affects the weights of linear layers and does not require calibration data or retraining, we can do this directly in PyTorch.\n",
    "\n",
    "We use the v1 student model, which was the best standard distillation student. We only quantize linear layers, because these nn.Linear layers account for most of the FLOPs and memory usage in inference, so quantizing them gives the biggest speed and size gains. Weights are static (don’t change at inference). As said, we use 8-bit integers, this should significantly increase speed.\n",
    "\n",
    "Since student_v1 is already trained and saved, we uantize only the classifier with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b062525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_v1_quant = copy.deepcopy(student_v1)\n",
    "\n",
    "quantized_classifier = torch.quantization.quantize_dynamic(\n",
    "    student_v1_quant.classifier,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8,\n",
    ")\n",
    "\n",
    "student_v1_quant.classifier = quantized_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f98ecc",
   "metadata": {},
   "source": [
    "**Evaluate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a0d5c81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Quantized Classifier] Accuracy: 0.8272, Precision: 0.8281, Recall: 0.8238\n"
     ]
    }
   ],
   "source": [
    "acc, prec, rec = evaluate(student_v1_quant, dataset, device=\"cpu\")\n",
    "print(f\"[Quantized Classifier] Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537956c7",
   "metadata": {},
   "source": [
    "Interpretation, see below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6734e",
   "metadata": {},
   "source": [
    "## b. Performance and Speed Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e96153c",
   "metadata": {},
   "source": [
    "We first evaluate the stuntent's models and then the teacher model. We evaluate their performance on exactly the same data.\n",
    "\n",
    "However, we need tot create a separate test dataset for the teacher that uses the teacher tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "949c54c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_teacher(example):\n",
    "    return teacher_tokenizer(example[\"question_content\"], truncation=True, padding=\"max_length\", max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c885d",
   "metadata": {},
   "source": [
    "Apply it to a fresh copy of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "07bcdfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d75b1be044340ae9fa4383f8c88b5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = Dataset.from_pandas(df_sample.copy())\n",
    "teacher_dataset = raw_dataset.map(tokenize_teacher, batched=True)\n",
    "teacher_dataset = teacher_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "teacher_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322ff88e",
   "metadata": {},
   "source": [
    "New dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "59658a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_test_loader = DataLoader(teacher_dataset[\"test\"], batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434be063",
   "metadata": {},
   "source": [
    "**Teacher vs best Models:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "332c4428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating teacher model\n",
      "Accuracy:  0.8744\n",
      "Precision: 0.8730\n",
      "Recall:    0.8720\n",
      "\n",
      "Evaluating the best student models\n",
      "[Standard] Accuracy: 0.6654, Precision: 0.6634, Recall: 0.6633\n",
      "[Intermediate] Accuracy: 0.6686, Precision: 0.6671, Recall: 0.6646\n",
      "[V1 Model] Accuracy: 0.8272, Precision: 0.8281, Recall: 0.8238\n",
      "[Quantized Classifier] Accuracy: 0.8272, Precision: 0.8281, Recall: 0.8238\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "print(\"\\nEvaluating teacher model\")\n",
    "\n",
    "teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in teacher_test_loader:\n",
    "        # Move all batch items to the same device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = teacher_model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch[\"label\"].cpu().numpy())\n",
    "\n",
    "# Compute and print metrics\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "prec = precision_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "rec = recall_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "\n",
    "# Log results\n",
    "results.append({\n",
    "    \"version\": \"teacher\",\n",
    "    \"accuracy\": acc,\n",
    "    \"precision\": prec,\n",
    "    \"recall\": rec\n",
    "})\n",
    "\n",
    "# Students:\n",
    "print(\"\\nEvaluating the best student models\")\n",
    "\n",
    "acc, prec, rec = evaluate(model_pretrained, dataset, device)\n",
    "print(f\"[Standard] Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}\")\n",
    "\n",
    "acc, prec, rec = evaluate(model_intermediate, dataset, device)\n",
    "print(f\"[Intermediate] Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}\")\n",
    "\n",
    "acc, prec, rec = evaluate(student_v1, dataset, device)\n",
    "print(f\"[V1 Model] Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}\")\n",
    "\n",
    "acc, prec, rec = evaluate(student_v1_quant, dataset, device=\"cpu\")\n",
    "print(f\"[Quantized Classifier] Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980fc1e7",
   "metadata": {},
   "source": [
    "The V1 model, trained for just 3 epochs with temperature scaling (temp.=2.0) and an equal balance between hard labels and soft teacher targets (alpha=0.5), achieves a validation accuracy of 82.72%, far surpassing both the best standard and intermediate student models.\n",
    "\n",
    "This is liely due to the fact that V1 was trained with soft targets from a high-performing BERT teacher (accuracy 87%), which provides nuanced information about class probabilities beyond the hard labels. This helped the student learn more than just the \"correct\" answer. Giving equal importance to the teacher’s logits and the ground truth labels allowed the student to learn both general patterns and dataset-specific decision boundaries. Softened logits exposed inter-class relationships. This richer signal likely improved generalization and robustness. V1 used a clean and focused setup without additional complexity (e.g., no intermediate layer matching), allowing the optimization to proceed without unnecessary distractions or noise in the objective. Even with only 5% of the full dataset, the training was effectively regularized by the teacher’s outputs. The limited data combined with soft labels reduced overfitting and allowed for meaningful generalization.\n",
    "\n",
    "For the Standard Distilled Student we had longer training, but worse performance, even after early stopping at epoch 7, this model achieved 16 percentage points lower accuracy than V1. It only relied on soft logits resulting in sharper distributions and weaker signals for secondary classes. The extended training didn't help much because the model lacked access to internal teacher knowledge and used low alpha, thereby underutilizing ground truth.\n",
    "\n",
    "The additional complexity only improved compared to the standard distilled student. This may have hurt generalization and we had rapid overfitting. This shows that more sophisticated objectives can be counterproductive without careful tuning or larger datasets.\n",
    "\n",
    "The quantified version achieves the same perfromance as the V1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42278e19",
   "metadata": {},
   "source": [
    "### Inference Speed Comparison\n",
    "\n",
    "We do a warm-up for stability.\n",
    "\n",
    "Sample 100 texts, we do this for all the best models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33425fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating teacher...\n",
      "Inference time on 100 samples (teacher): 0.7009 seconds\n",
      "\n",
      "Evaluating student_best_pretrained...\n",
      "Inference time on 100 samples (student_best_pretrained): 0.3438 seconds\n",
      "\n",
      "Evaluating student_best_intermediate...\n",
      "Inference time on 100 samples (student_best_intermediate): 0.3357 seconds\n",
      "\n",
      "Evaluating student_v1...\n",
      "Inference time on 100 samples (student_v1): 0.3371 seconds\n",
      "\n",
      "Evaluating student_v1_quantized...\n",
      "Inference time on 100 samples (student_v1_quantized): 0.3482 seconds\n"
     ]
    }
   ],
   "source": [
    "texts = df_sample[\"question_content\"].tolist()[:100]\n",
    "batch_size = 16  # Reduce if still getting OOM\n",
    "\n",
    "# model paths\n",
    "model_variants = {\n",
    "    \"teacher\": {\n",
    "        \"model\": teacher_model,\n",
    "        \"tokenizer\": teacher_tokenizer\n",
    "    },\n",
    "    \"student_best_pretrained\": {\n",
    "        \"model_path\": \"models/student_best_pretrained\"\n",
    "    },\n",
    "    \"student_best_intermediate\": {\n",
    "        \"model_path\": \"models/student_best_intermediate\"\n",
    "    },\n",
    "    \"student_v1\": {\n",
    "        \"model\": student_v1,\n",
    "        \"tokenizer\": student_tokenizer\n",
    "    },\n",
    "    \"student_v1_quantized\": {\n",
    "        \"model\": student_v1_quant,\n",
    "        \"tokenizer\": student_tokenizer\n",
    "    }\n",
    "}\n",
    "\n",
    "# Loop through each model version:\n",
    "for name, info in model_variants.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "\n",
    "    # Load model/tokenizer\n",
    "    if \"model\" in info and \"tokenizer\" in info:\n",
    "        model = info[\"model\"].to(device)\n",
    "        tokenizer = info[\"tokenizer\"]\n",
    "    else:\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(info[\"model_path\"]).to(device)\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained(info[\"model_path\"])\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize all texts in advance (on CPU)\n",
    "    encoded = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Move tensors to device in batches\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = {k: v[i:i+batch_size].to(device) for k, v in encoded.items()}\n",
    "            _ = model(**batch)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Inference time on 100 samples ({name}): {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "    # Optional: clear memory between models\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e0ea0",
   "metadata": {},
   "source": [
    "We observe that the teacher model takes the longest to process 100 samples, with an inference time of approximately 0.70 seconds, reflecting its larger size and complexity. In contrast, all student variants, including the pretrained, intermediate, v1, and quantized versions, achieve significantly faster inference times around 0.33 to 0.35 seconds, demonstrating the efficiency gains achieved through distillation and model compression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfa75b",
   "metadata": {},
   "source": [
    "## c. Analysis and Improvements\n",
    "\n",
    "Last, we are supposed to analyze deficiencies in the student model's learning. Suggest potential improvements or further research directions.\n",
    "\n",
    "The student models that we chose were limited to the DistilBERT base architecture for the case when we do not want to retrain and use the weights. We saw that already a very simple approach can give very good performance, while benefitting from reduced computaional inference time. \n",
    "\n",
    "Even though the models benefit from faster inference and are much faster to be trained one has to consider that the training still takes a signifcant amount of time which leads to the fact that one can even with those models only do some limited parameter tuning.\n",
    "\n",
    "The most obvious deficiency of the student model's learning is the limited model capacity. Fewer parameters (fewer layers, smaller hidden size, fewer attention heads) mean that it cannot represent complex functions learned by the teacher. Thus, the student may underfit especially on nuanced or ambiguous samples.\n",
    "\n",
    "While the student may generalize quite good, the student misses depth which makes it worse at modeling complex syntax or abstract semantics.\n",
    "\n",
    "There could be models that have different number of layers that do not require retraining everything from scratch.\n",
    "\n",
    "We also suggest experimenting with more advanced quantization techniques, such as post-training quantization with ONNX Runtime or quantization-aware training, which may further reduce memory and inference costs with minimal accuracy degradation. Finally, better distillation objectives that combine soft targets with intermediate layer alignment or contrastive learning may allow student models to capture richer representations without increasing model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ceba55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 10:\n",
      "Text: can anyone tell me the medical condition that was on Melbournes all saints on the tuesday the 23td of may thanks\n",
      "True Label:     3\n",
      "Teacher Pred:   3 \n",
      "Student V1 Pred:0 \n"
     ]
    }
   ],
   "source": [
    "def predict(model, tokenizer, texts, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            probs = softmax(outputs.logits, dim=1)\n",
    "            pred = torch.argmax(probs, dim=1).item()\n",
    "            preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "# Predict using teacher and student_v1\n",
    "teacher_preds = predict(teacher_model, teacher_tokenizer, texts, device)\n",
    "student_preds = predict(student_v1, student_tokenizer, texts, device)\n",
    "\n",
    "# Compare and print misclassified examples\n",
    "for i, (true, t_pred, s_pred) in enumerate(zip(labels, teacher_preds, student_preds)):\n",
    "    if t_pred == true and s_pred != true:\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"Text: {texts[i]}\")\n",
    "        print(f\"True Label:     {true}\")\n",
    "        print(f\"Teacher Pred:   {t_pred} \")\n",
    "        print(f\"Student V1 Pred:{s_pred} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b97901",
   "metadata": {},
   "source": [
    "In this example, the teacher model correctly identifies the topic as class 3, likely a medical or health-related category, while the student V1 model misclassifies it as class 0. The text refers to a medical condition on a TV show, which may require more nuanced understanding of context and domain-specific terms. This suggests that the student model, due to its reduced capacity, struggles to capture such specific references and semantic cues, leading to errors in more specialized or context-dependent queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
